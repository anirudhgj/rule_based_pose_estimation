{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mcode started\u001b[0m\n",
      "\u001b[34mloading weights\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from ../../../pose_embedding_train_full_view_norm_range_one/ent44_15j_32/weights/encoder_iter-799001\n",
      "\u001b[33mloaded pose_encoder weights\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from ../../../pose_embedding_train_full_view_norm_range_one/ent44_15j_32/weights/decoder_iter-799001\n",
      "\u001b[34mloaded pose_decoder weights\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from ../../../motion_embedding_train/ent_44_15j_32/BILSTM_TRAIN/weights/lstm_encoder/rule_based_motion_net_expt_seq309100\n",
      "\u001b[32mloaded pose lstm_encoder weights\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from ../../../motion_embedding_train/ent_44_15j_32/BILSTM_TRAIN/weights/lstm_decoder/rule_based_motion_net_expt_seq309100\n",
      "\u001b[31mlaoded pose lstm_decoder weights\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import numpy as np\n",
    "from numpy import pi\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import cv2\n",
    "import logging,argparse\n",
    "from data_loader import Data_loader\n",
    "import model \n",
    "from hyperparams import Hyperparameters\n",
    "import utils\n",
    "from commons import tf_transform\n",
    "import graph\n",
    "from termcolor import colored \n",
    "import model_componets as comps\n",
    "from commons import transform_util\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print colored(\"code started\",\"red\")\n",
    "\n",
    "H = Hyperparameters ()\n",
    "\n",
    "D = Data_loader(H.data_path,H.seq_length,H.batch_size)\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(0)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "input_ph = tf.placeholder(tf.float32, shape= [None , H.num_joints ,3],name = 'skeleton_input')\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "x_input = input_ph\n",
    "\n",
    "x_input_view_norm,x_input_local,tr_mats = comps.root_relative_to_local(x_input)\n",
    "\n",
    "encoder_out = graph.apply_pose_encoder(input_ph)\n",
    "\n",
    "pose_encoder_params = graph.get_network_params(\"Encoder_net\")\n",
    "\n",
    "encoder_input = tf.reshape(encoder_out,(-1,H.seq_length,32))\n",
    "\n",
    "encoder_lstm_out = model.apply_encoder(encoder_input,name ='motion_encoder')\n",
    "\n",
    "z_state = encoder_lstm_out['z_state']\n",
    "\n",
    "z_outputs = encoder_lstm_out['z_outputs']\n",
    "\n",
    "decoder_lstm_out = model.apply_decoder(z_state,z_outputs,name = 'motion_decoder')\n",
    "\n",
    "motion_recon = decoder_lstm_out['x_recon']\n",
    "\n",
    "motion_recon_reshaped = tf.reshape(motion_recon,((-1,32)))\n",
    "\n",
    "pose_recon = graph.apply_pose_decoder(motion_recon_reshaped)#view norm\n",
    "\n",
    "pose_decoder_params = graph.get_network_params(\"Decoder_net\")\n",
    "\n",
    "loss_motion = tf.reduce_mean((pose_recon - x_input_view_norm)**2)\n",
    "\n",
    "loss_motion_summary = tf.summary.scalar('loss',loss_motion)\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "param_lstm_encoder = model.get_network_params('motion_encoder')\n",
    "\n",
    "param_lstm_decoder = model.get_network_params('motion_decoder')\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print colored(\"loading weights\",\"blue\")\n",
    "\n",
    "tf.train.Saver(pose_encoder_params).restore(sess,'../../../pose_embedding_train_full_view_norm_range_one/ent44_15j_32/weights/encoder_iter-799001') \n",
    "print colored(\"loaded pose_encoder weights\",\"yellow\")\n",
    "\n",
    "tf.train.Saver(pose_decoder_params).restore(sess,'../../../pose_embedding_train_full_view_norm_range_one/ent44_15j_32/weights/decoder_iter-799001')\n",
    "print colored(\"loaded pose_decoder weights\",\"blue\")\n",
    "\n",
    "tf.train.Saver(param_lstm_encoder).restore(sess,tf.train.latest_checkpoint('../../../motion_embedding_train/ent_44_15j_32/BILSTM_TRAIN/weights/lstm_encoder/'))\n",
    "print colored(\"loaded pose lstm_encoder weights\",\"green\")\n",
    "\n",
    "tf.train.Saver(param_lstm_decoder).restore(sess,tf.train.latest_checkpoint('../../../motion_embedding_train/ent_44_15j_32/BILSTM_TRAIN/weights/lstm_decoder/'))\n",
    "print colored(\"laoded pose lstm_decoder weights\",\"red\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from other_utils import np_utils, sk_utils\n",
    "import imageio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = np.asarray(D.get_sequence_batch_train())\n",
    "\n",
    "train_batch = train_batch[:,0:30]\n",
    "\n",
    "train_batch = np.reshape(utils.augment_pose_seq(train_batch),(-1,H.num_joints * 3))\n",
    "\n",
    "train_batch = train_batch.reshape((-1,H.num_joints , 3))\n",
    "\n",
    "angles,train_batch_view_norm,train_batch_local = transform_util.root_relative_to_local_skeleton_batch(train_batch)\n",
    "\n",
    "feed_dict = {x_input : train_batch}\n",
    "\n",
    "op_train_dict = sess.run({'embedding':z_state, 'final_pose': pose_recon}, feed_dict=feed_dict)\n",
    "\n",
    "predictions = op_train_dict['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128,)\n",
      "('minimum vaue of mean ', -1.2129349)\n",
      "('minimum value of std ', 0.34521836)\n",
      "('maximum value of mean', 1.7195117)\n",
      "('maximum value of std', 1.5960555)\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(predictions, axis=0)\n",
    "std = np.std(predictions,axis=0)\n",
    "\n",
    "print mean.shape\n",
    "print std.shape\n",
    "\n",
    "# x_pred = dms.decode(predictions)\n",
    "# x_pred.shape\n",
    "\n",
    "# imgs = sk_utils.get_skeleton_images(x_pred[0], title_prefix='p')\n",
    "# len(imgs)\n",
    "print('minimum vaue of mean ', min(mean))\n",
    "print('minimum value of std ', min(std))\n",
    "print('maximum value of mean', max(mean))\n",
    "print('maximum value of std', max(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = mean+(2*std)\n",
    "sample_2 = mean-(2*std)\n",
    "sample_1 = np.reshape(sample_1 , (1,128))\n",
    "sample_2 = np.reshape(sample_2 , (1,128))\n",
    "map_d = 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(0,1, map_d)\n",
    "a = np.reshape(a, (map_d,1))\n",
    "\n",
    "\n",
    "vertical = a*sample_1[:,:64] + (1-a)*sample_2[:,:64]\n",
    "\n",
    "assert (vertical[0] == sample_2[0,:64]).all()\n",
    "\n",
    "\n",
    "horizontal = a*sample_1[:,64:] + (1-a)*sample_2[:,64:]\n",
    "\n",
    "assert (horizontal[0] == sample_2[0,64:]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 120, 128)\n",
      "(14400, 128)\n"
     ]
    }
   ],
   "source": [
    "grid = np.empty((map_d,map_d,128))\n",
    "\n",
    "for i in range(map_d):\n",
    "    for j in range(map_d):\n",
    "        grid[i,j] = np.concatenate((vertical[i], horizontal[j]))\n",
    "        \n",
    "grid_flat = np.reshape(grid, (-1,128))\n",
    "\n",
    "print grid.shape\n",
    "print grid_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ph = tf.placeholder(tf.float32 , shape = [None ,128],name = \"embed\")\n",
    "\n",
    "final_state_fced_stacked = tf.stack([embed_ph]*30, 1)\n",
    "\n",
    "\n",
    "decoder_lstm_out = model.apply_decoder(z_state,z_outputs,name = 'motion_decoder')\n",
    "\n",
    "motion_recon = decoder_lstm_out['x_recon']\n",
    "\n",
    "motion_recon_reshaped = tf.reshape(motion_recon,((-1,32)))\n",
    "\n",
    "pose_recon = graph.apply_pose_decoder(motion_recon_reshaped)#view norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
