{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mcode started\u001b[0m\n",
      "WARNING:tensorflow:From /virtual_envs/ani_python2.7/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From model_componets.py:33: calling norm (from tensorflow.python.ops.linalg_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From commons/tf_transform.py:21: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From utils.py:36: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From model.py:15: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From model.py:22: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /virtual_envs/ani_python2.7/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "\u001b[34mloading weights\u001b[0m\n",
      "WARNING:tensorflow:From /virtual_envs/ani_python2.7/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ../../../pose_embedding_train_full_view_norm_range_one/ent44_15j_32/weights/encoder_iter-799001\n",
      "\u001b[33mloaded pose_encoder weights\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from ../../../pose_embedding_train_full_view_norm_range_one/ent44_15j_32/weights/decoder_iter-799001\n",
      "\u001b[34mloaded pose_decoder weights\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from ../../../motion_embedding_train/ent_44_15j_32/BILSTM_TRAIN/backup/weights/lstm_encoder/rule_based_motion_net_expt_seq309100\n",
      "\u001b[32mloaded pose lstm_encoder weights\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from ../../../motion_embedding_train/ent_44_15j_32/BILSTM_TRAIN/backup/weights/lstm_decoder/rule_based_motion_net_expt_seq309100\n",
      "\u001b[31mloaded pose lstm_decoder weights\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import numpy as np\n",
    "from numpy import pi\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import cv2\n",
    "import logging,argparse\n",
    "from data_loader import Data_loader\n",
    "import model \n",
    "from hyperparams import Hyperparameters\n",
    "import utils\n",
    "from commons import tf_transform\n",
    "import graph\n",
    "from termcolor import colored \n",
    "import model_componets as comps\n",
    "from commons import transform_util\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print colored(\"code started\",\"red\")\n",
    "\n",
    "H = Hyperparameters ()\n",
    "\n",
    "D = Data_loader(H.data_path,H.seq_length,H.batch_size)\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(1)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "input_ph = tf.placeholder(tf.float32, shape= [None , H.num_joints ,3],name = 'skeleton_input')\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "x_input = input_ph\n",
    "\n",
    "x_input_view_norm,x_input_local,tr_mats = comps.root_relative_to_local(x_input)\n",
    "\n",
    "encoder_out = graph.apply_pose_encoder(input_ph)\n",
    "\n",
    "pose_encoder_params = graph.get_network_params(\"Encoder_net\")\n",
    "\n",
    "encoder_input = tf.reshape(encoder_out,(-1,H.seq_length,32))\n",
    "\n",
    "encoder_lstm_out = model.apply_encoder(encoder_input,name ='motion_encoder')\n",
    "\n",
    "z_state = encoder_lstm_out['z_state']\n",
    "\n",
    "z_outputs = encoder_lstm_out['z_outputs']\n",
    "\n",
    "decoder_lstm_out = model.apply_decoder(z_state,z_outputs,name = 'motion_decoder')\n",
    "\n",
    "motion_recon = decoder_lstm_out['x_recon']\n",
    "\n",
    "motion_recon_reshaped = tf.reshape(motion_recon,((-1,32)))\n",
    "\n",
    "pose_recon = graph.apply_pose_decoder(motion_recon_reshaped)#view norm\n",
    "\n",
    "pose_decoder_params = graph.get_network_params(\"Decoder_net\")\n",
    "\n",
    "param_lstm_encoder = model.get_network_params('motion_encoder')\n",
    "\n",
    "param_lstm_decoder = model.get_network_params('motion_decoder')\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print colored(\"loading weights\",\"blue\")\n",
    "\n",
    "tf.train.Saver(pose_encoder_params).restore(sess,'../../../pose_embedding_train_full_view_norm_range_one/ent44_15j_32/weights/encoder_iter-799001') \n",
    "print colored(\"loaded pose_encoder weights\",\"yellow\")\n",
    "\n",
    "tf.train.Saver(pose_decoder_params).restore(sess,'../../../pose_embedding_train_full_view_norm_range_one/ent44_15j_32/weights/decoder_iter-799001')\n",
    "print colored(\"loaded pose_decoder weights\",\"blue\")\n",
    "\n",
    "tf.train.Saver(param_lstm_encoder).restore(sess,tf.train.latest_checkpoint('../../../motion_embedding_train/ent_44_15j_32/BILSTM_TRAIN/backup/weights/lstm_encoder/'))\n",
    "print colored(\"loaded pose lstm_encoder weights\",\"green\")\n",
    "\n",
    "tf.train.Saver(param_lstm_decoder).restore(sess,tf.train.latest_checkpoint('../../../motion_embedding_train/ent_44_15j_32/BILSTM_TRAIN/backup/weights/lstm_decoder/'))\n",
    "print colored(\"loaded pose lstm_decoder weights\",\"red\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import imageio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 128)\n",
      "[[[ 0.          0.          0.        ]\n",
      "  [ 1.07804064 -0.05362856  4.62573803]\n",
      "  [ 2.31093687  0.44475523  4.29637426]\n",
      "  ...\n",
      "  [-0.90933925 -0.40141874  0.33832694]\n",
      "  [-1.34540683 -0.53031303 -3.83698518]\n",
      "  [-1.59571529 -1.24935613 -7.35555345]]\n",
      "\n",
      " [[ 0.          0.          0.        ]\n",
      "  [ 1.08808855 -0.0228007   4.62363963]\n",
      "  [ 2.31454163  0.4921511   4.29571054]\n",
      "  ...\n",
      "  [-0.90105966 -0.41898949  0.33917444]\n",
      "  [-1.33174691 -0.57619251 -3.83572624]\n",
      "  [-1.56383154 -1.32998974 -7.34826471]]\n",
      "\n",
      " [[ 0.          0.          0.        ]\n",
      "  [ 1.1083911   0.02832133  4.61878415]\n",
      "  [ 2.32163795  0.57540191  4.29376886]\n",
      "  ...\n",
      "  [-0.88419834 -0.44966805  0.34422658]\n",
      "  [-1.29859235 -0.67310359 -3.8293036 ]\n",
      "  [-1.49425045 -1.50399169 -7.32663728]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.          0.          0.        ]\n",
      "  [ 1.68134831  0.14084152  4.44024003]\n",
      "  [ 2.65165432  0.85145295  3.78416256]\n",
      "  ...\n",
      "  [-0.89801277 -0.50989503  0.18994767]\n",
      "  [-2.02721347  0.46232451 -3.73684427]\n",
      "  [-2.29848853  0.15054691 -7.31304401]]\n",
      "\n",
      " [[ 0.          0.          0.        ]\n",
      "  [ 1.65860432  0.13326515  4.44901923]\n",
      "  [ 2.62302796  0.86035535  3.80239191]\n",
      "  ...\n",
      "  [-0.87446024 -0.556297    0.16838329]\n",
      "  [-1.96245172  0.31475354 -3.79362952]\n",
      "  [-2.18746225 -0.05192009 -7.36783163]]\n",
      "\n",
      " [[ 0.          0.          0.        ]\n",
      "  [ 1.64984599  0.12577367  4.45249247]\n",
      "  [ 2.61216666  0.86033396  3.8111968 ]\n",
      "  ...\n",
      "  [-0.86122927 -0.57877005  0.16065296]\n",
      "  [-1.92339182  0.23043489 -3.82143168]\n",
      "  [-2.13231765 -0.151772   -7.39498284]]]\n"
     ]
    }
   ],
   "source": [
    "train_batch = np.asarray(D.get_sequence_batch_train())\n",
    "\n",
    "train_batch = train_batch[:,0:30]\n",
    "\n",
    "train_batch = np.reshape(utils.augment_pose_seq(train_batch),(-1,H.num_joints * 3))\n",
    "\n",
    "train_batch = train_batch.reshape((-1,H.num_joints , 3))\n",
    "\n",
    "angles,train_batch_view_norm,train_batch_local = transform_util.root_relative_to_local_skeleton_batch(train_batch)\n",
    "\n",
    "feed_dict = {x_input : train_batch}\n",
    "\n",
    "op_train_dict = sess.run({'embedding':z_state, 'final_pose': pose_recon}, feed_dict=feed_dict)\n",
    "\n",
    "predictions = op_train_dict['embedding']\n",
    "\n",
    "print predictions.shape\n",
    "print train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128,)\n",
      "('minimum vaue of mean ', -3.438107)\n",
      "('minimum value of std ', 0.3991607)\n",
      "('maximum value of mean', 4.8164005)\n",
      "('maximum value of std', 1.8936635)\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(predictions, axis=0)\n",
    "std = np.std(predictions,axis=0)\n",
    "\n",
    "print mean.shape\n",
    "print std.shape\n",
    "\n",
    "# x_pred = dms.decode(predictions)\n",
    "# x_pred.shape\n",
    "\n",
    "# imgs = sk_utils.get_skeleton_images(x_pred[0], title_prefix='p')\n",
    "# len(imgs)\n",
    "print('minimum vaue of mean ', min(mean))\n",
    "print('minimum value of std ', min(std))\n",
    "print('maximum value of mean', max(mean))\n",
    "print('maximum value of std', max(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = mean+(4*std)\n",
    "sample_2 = mean-(4*std)\n",
    "sample_1 = np.reshape(sample_1 , (1,128))\n",
    "sample_2 = np.reshape(sample_2 , (1,128))\n",
    "map_d = 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(0,1, map_d)\n",
    "a = np.reshape(a, (map_d,1))\n",
    "\n",
    "\n",
    "vertical = a*sample_1[:,:64] + (1-a)*sample_2[:,:64]\n",
    "\n",
    "assert (vertical[0] == sample_2[0,:64]).all()\n",
    "\n",
    "\n",
    "horizontal = a*sample_1[:,64:] + (1-a)*sample_2[:,64:]\n",
    "\n",
    "assert (horizontal[0] == sample_2[0,64:]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 120, 128)\n",
      "(14400, 128)\n"
     ]
    }
   ],
   "source": [
    "grid = np.empty((map_d,map_d,128))\n",
    "\n",
    "for i in range(map_d):\n",
    "    for j in range(map_d):\n",
    "        grid[i,j] = np.concatenate((vertical[i], horizontal[j]))\n",
    "        \n",
    "grid_flat = np.reshape(grid, (-1,128))\n",
    "print grid.shape\n",
    "print grid_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/virtual_envs/ani_python2.7/local/lib/python2.7/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mloading weights\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from ../../../pose_embedding_train_full_view_norm_range_one/ent44_15j_32/weights/decoder_iter-799001\n",
      "\u001b[34mloaded pose_decoder weights\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from ./backup/weights/lstm_decoder/rule_based_motion_net_expt_seq309100\n",
      "\u001b[31mloaded pose lstm_decoder weights\u001b[0m\n",
      "[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 1.45241022e-01 -4.09731176e-03  4.74777699e+00]\n",
      "  [ 1.48487914e+00 -2.69730359e-01  4.85598135e+00]\n",
      "  ...\n",
      "  [-1.04785395e+00  6.65438622e-02  8.58378876e-03]\n",
      "  [-1.22207785e+00  2.33827233e+00 -3.51971388e+00]\n",
      "  [-2.21926928e+00  1.09298229e+00 -6.74691772e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 1.39605477e-01 -3.46905057e-04  4.74794769e+00]\n",
      "  [ 1.44601071e+00 -4.05614167e-01  4.82516909e+00]\n",
      "  ...\n",
      "  [-1.04791236e+00  6.47635758e-02  1.36066545e-02]\n",
      "  [-1.88592088e+00  2.62795186e+00 -3.20629978e+00]\n",
      "  [-1.39344954e+00 -6.14766836e-01 -4.69028997e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 1.33441702e-01 -3.19332030e-04  4.74812555e+00]\n",
      "  [ 1.45433378e+00 -3.50480884e-01  4.84574938e+00]\n",
      "  ...\n",
      "  [-1.04777062e+00  6.69847205e-02  1.37649141e-02]\n",
      "  [-1.45088375e+00  2.48625803e+00 -3.39572287e+00]\n",
      "  [-1.40245652e+00 -3.94644499e-01 -5.55397081e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 4.95902896e-02 -2.91247405e-02  4.74965191e+00]\n",
      "  [ 1.36851835e+00  1.65145814e-01  5.06522560e+00]\n",
      "  ...\n",
      "  [-1.01803386e+00  4.82067317e-02 -2.52552867e-01]\n",
      "  [-1.01803386e+00  4.82067317e-02 -2.52552867e-01]\n",
      "  [-1.01803386e+00  4.82067317e-02 -2.52552867e-01]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 3.62934470e-02 -3.15226093e-02  4.74975681e+00]\n",
      "  [ 1.35144639e+00  1.75222576e-01  5.07306576e+00]\n",
      "  ...\n",
      "  [-1.01818728e+00  4.52795103e-02 -2.52475858e-01]\n",
      "  [-1.01818728e+00  4.52795103e-02 -2.52475858e-01]\n",
      "  [-1.01818728e+00  4.52795103e-02 -2.52475858e-01]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 9.92881320e-03 -3.10884658e-02  4.74988794e+00]\n",
      "  [ 1.33839142e+00  1.76762447e-01  5.01234627e+00]\n",
      "  ...\n",
      "  [-1.02142680e+00  3.18498909e-02 -2.41189942e-01]\n",
      "  [-1.02142680e+00  3.18498909e-02 -2.41189942e-01]\n",
      "  [-1.02142680e+00  3.18498909e-02 -2.41189942e-01]]]\n"
     ]
    }
   ],
   "source": [
    "sess1 = tf.InteractiveSession(config=config)\n",
    "\n",
    "embed_ph = tf.placeholder(tf.float32 , shape = [None ,128],name = \"embed\")\n",
    "\n",
    "final_state_fced_stacked = tf.stack([embed_ph]*30, 1)\n",
    "\n",
    "decoder_lstm_out = model.apply_decoder(embed_ph,final_state_fced_stacked,name = 'motion_decoder')\n",
    "\n",
    "motion_recon = decoder_lstm_out['x_recon']\n",
    "\n",
    "motion_recon_reshaped = tf.reshape(motion_recon,((-1,32)))\n",
    "\n",
    "pose_recon1 = graph.apply_pose_decoder(motion_recon_reshaped)#view norm\n",
    "\n",
    "param_lstm_decoder1 = model.get_network_params('motion_decoder')\n",
    "\n",
    "pose_decoder_params1 = graph.get_network_params(\"Decoder_net\")\n",
    "\n",
    "# sess1.run(tf.global_variables_initializer())\n",
    "\n",
    "print colored(\"loading weights\",\"blue\")\n",
    "\n",
    "tf.train.Saver(pose_decoder_params).restore(sess1,'../../../pose_embedding_train_full_view_norm_range_one/ent44_15j_32/weights/decoder_iter-799001')\n",
    "print colored(\"loaded pose_decoder weights\",\"blue\")\n",
    "\n",
    "tf.train.Saver(param_lstm_decoder1).restore(sess1,tf.train.latest_checkpoint('./backup/weights/lstm_decoder/'))\n",
    "print colored(\"loaded pose lstm_decoder weights\",\"red\")\n",
    "\n",
    "poses_for_grid = sess1.run(pose_recon1,feed_dict = {embed_ph : grid_flat})\n",
    "\n",
    "print poses_for_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions =  poses_for_grid.reshape(-1,30,15,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14400, 30)\n",
      "(14400, 29)\n",
      "(120, 120)\n"
     ]
    }
   ],
   "source": [
    "predictions = np.mean(predictions, axis=-1)\n",
    "predictions = np.mean(predictions, axis=-1)\n",
    "\n",
    "print predictions.shape\n",
    "\n",
    "velocity = predictions[:,1:] - predictions[:,:-1]\n",
    "speed = np.absolute(velocity)\n",
    "\n",
    "print(speed.shape)\n",
    "\n",
    "speed  = np.mean(speed, axis=-1)\n",
    "\n",
    "speed = np.reshape(speed, (map_d,map_d))\n",
    "\n",
    "print(speed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.imshow(speed, cmap='hot', interpolation='nearest')\n",
    "# plt.savefig(\"test.svg\", format=\"svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02705879 0.027028   0.02700059 ... 0.0378325  0.03764719 0.0374699 ]\n",
      " [0.02692368 0.02690491 0.02688072 ... 0.03778518 0.03760965 0.03744886]\n",
      " [0.02681354 0.0267837  0.02676796 ... 0.03772433 0.03758414 0.03740972]\n",
      " ...\n",
      " [0.00296076 0.00297431 0.00298757 ... 0.00749226 0.00748829 0.00747871]\n",
      " [0.00295593 0.00297051 0.00298556 ... 0.00744012 0.00743131 0.00742238]\n",
      " [0.00295224 0.0029674  0.00298287 ... 0.00738546 0.00737506 0.00736344]]\n"
     ]
    }
   ],
   "source": [
    "print speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
