{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import numpy as np\n",
    "from numpy import pi\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import cv2\n",
    "import logging,argparse\n",
    "from data_loader import Data_loader\n",
    "import model \n",
    "from hyperparams import Hyperparameters\n",
    "import utils\n",
    "from commons import tf_transform\n",
    "import graph\n",
    "from termcolor import colored \n",
    "import model_componets as comps\n",
    "from commons import transform_util\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print colored(\"code started\",\"red\")\n",
    "\n",
    "H = Hyperparameters ()\n",
    "\n",
    "D = Data_loader(H.data_path,H.seq_length,H.batch_size)\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(1)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "input_ph = tf.placeholder(tf.float32, shape= [None , H.num_joints ,3],name = 'skeleton_input')\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "x_input = input_ph\n",
    "\n",
    "x_input_view_norm,x_input_local,tr_mats = comps.root_relative_to_local(x_input)\n",
    "\n",
    "encoder_out = graph.apply_pose_encoder(input_ph)\n",
    "\n",
    "pose_encoder_params = graph.get_network_params(\"Encoder_net\")\n",
    "\n",
    "encoder_input = tf.reshape(encoder_out,(-1,H.seq_length,32))\n",
    "\n",
    "encoder_lstm_out = model.apply_encoder(encoder_input,name ='motion_encoder')\n",
    "\n",
    "z_state = encoder_lstm_out['z_state']\n",
    "\n",
    "z_outputs = encoder_lstm_out['z_outputs']\n",
    "\n",
    "decoder_lstm_out = model.apply_decoder(z_state,z_outputs,name = 'motion_decoder')\n",
    "\n",
    "motion_recon = decoder_lstm_out['x_recon']\n",
    "\n",
    "motion_recon_reshaped = tf.reshape(motion_recon,((-1,32)))\n",
    "\n",
    "pose_recon = graph.apply_pose_decoder(motion_recon_reshaped)#view norm\n",
    "\n",
    "pose_decoder_params = graph.get_network_params(\"Decoder_net\")\n",
    "\n",
    "param_lstm_encoder = model.get_network_params('motion_encoder')\n",
    "\n",
    "param_lstm_decoder = model.get_network_params('motion_decoder')\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print colored(\"loading weights\",\"blue\")\n",
    "\n",
    "tf.train.Saver(pose_encoder_params).restore(sess,'../../../pose_embedding_train_full_view_norm_range_one/ent44_15j_32/weights/encoder_iter-799001') \n",
    "print colored(\"loaded pose_encoder weights\",\"yellow\")\n",
    "\n",
    "tf.train.Saver(pose_decoder_params).restore(sess,'../../../pose_embedding_train_full_view_norm_range_one/ent44_15j_32/weights/decoder_iter-799001')\n",
    "print colored(\"loaded pose_decoder weights\",\"blue\")\n",
    "\n",
    "tf.train.Saver(param_lstm_encoder).restore(sess,tf.train.latest_checkpoint('../../../motion_embedding_train/ent_44_15j_32/BILSTM_TRAIN/backup/weights/lstm_encoder/'))\n",
    "print colored(\"loaded pose lstm_encoder weights\",\"green\")\n",
    "\n",
    "tf.train.Saver(param_lstm_decoder).restore(sess,tf.train.latest_checkpoint('../../../motion_embedding_train/ent_44_15j_32/BILSTM_TRAIN/backup/weights/lstm_decoder/'))\n",
    "print colored(\"loaded pose lstm_decoder weights\",\"red\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import imageio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = np.asarray(D.get_sequence_batch_train())\n",
    "\n",
    "train_batch = train_batch[:,0:30]\n",
    "\n",
    "train_batch = np.reshape(utils.augment_pose_seq(train_batch),(-1,H.num_joints * 3))\n",
    "\n",
    "train_batch = train_batch.reshape((-1,H.num_joints , 3))\n",
    "\n",
    "angles,train_batch_view_norm,train_batch_local = transform_util.root_relative_to_local_skeleton_batch(train_batch)\n",
    "\n",
    "feed_dict = {x_input : train_batch}\n",
    "\n",
    "op_train_dict = sess.run({'embedding':z_state, 'final_pose': pose_recon}, feed_dict=feed_dict)\n",
    "\n",
    "predictions = op_train_dict['embedding']\n",
    "\n",
    "print train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(predictions, axis=0)\n",
    "std = np.std(predictions,axis=0)\n",
    "\n",
    "print mean.shape\n",
    "print std.shape\n",
    "\n",
    "# x_pred = dms.decode(predictions)\n",
    "# x_pred.shape\n",
    "\n",
    "# imgs = sk_utils.get_skeleton_images(x_pred[0], title_prefix='p')\n",
    "# len(imgs)\n",
    "print('minimum vaue of mean ', min(mean))\n",
    "print('minimum value of std ', min(std))\n",
    "print('maximum value of mean', max(mean))\n",
    "print('maximum value of std', max(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = mean+(4*std)\n",
    "sample_2 = mean-(4*std)\n",
    "sample_1 = np.reshape(sample_1 , (1,128))\n",
    "sample_2 = np.reshape(sample_2 , (1,128))\n",
    "map_d = 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(0,1, map_d)\n",
    "a = np.reshape(a, (map_d,1))\n",
    "\n",
    "\n",
    "vertical = a*sample_1[:,:64] + (1-a)*sample_2[:,:64]\n",
    "\n",
    "assert (vertical[0] == sample_2[0,:64]).all()\n",
    "\n",
    "\n",
    "horizontal = a*sample_1[:,64:] + (1-a)*sample_2[:,64:]\n",
    "\n",
    "assert (horizontal[0] == sample_2[0,64:]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.empty((map_d,map_d,128))\n",
    "\n",
    "for i in range(map_d):\n",
    "    for j in range(map_d):\n",
    "        grid[i,j] = np.concatenate((vertical[i], horizontal[j]))\n",
    "        \n",
    "grid_flat = np.reshape(grid, (-1,128))\n",
    "print grid.shape\n",
    "print grid_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess1 = tf.InteractiveSession(config=config)\n",
    "\n",
    "embed_ph = tf.placeholder(tf.float32 , shape = [None ,128],name = \"embed\")\n",
    "\n",
    "final_state_fced_stacked = tf.stack([embed_ph]*30, 1)\n",
    "\n",
    "decoder_lstm_out = model.apply_decoder(embed_ph,final_state_fced_stacked,name = 'motion_decoder')\n",
    "\n",
    "motion_recon = decoder_lstm_out['x_recon']\n",
    "\n",
    "motion_recon_reshaped = tf.reshape(motion_recon,((-1,32)))\n",
    "\n",
    "pose_recon1 = graph.apply_pose_decoder(motion_recon_reshaped)#view norm\n",
    "\n",
    "param_lstm_decoder1 = model.get_network_params('motion_decoder')\n",
    "\n",
    "pose_decoder_params1 = graph.get_network_params(\"Decoder_net\")\n",
    "\n",
    "# sess1.run(tf.global_variables_initializer())\n",
    "\n",
    "print colored(\"loading weights\",\"blue\")\n",
    "\n",
    "tf.train.Saver(pose_decoder_params).restore(sess1,'../../../pose_embedding_train_full_view_norm_range_one/ent44_15j_32/weights/decoder_iter-799001')\n",
    "print colored(\"loaded pose_decoder weights\",\"blue\")\n",
    "\n",
    "tf.train.Saver(param_lstm_decoder1).restore(sess1,tf.train.latest_checkpoint('../../../motion_embedding_train/ent_44_15j_32/BILSTM_TRAIN/backup/weights/lstm_decoder/'))\n",
    "print colored(\"loaded pose lstm_decoder weights\",\"red\")\n",
    "\n",
    "poses_for_grid = sess1.run(pose_recon1,feed_dict = {embed_ph : grid_flat})\n",
    "\n",
    "print poses_for_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions =  poses_for_grid.reshape(-1,30,15,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.mean(predictions, axis=-1)\n",
    "predictions = np.mean(predictions, axis=-1)\n",
    "\n",
    "print predictions.shape\n",
    "\n",
    "velocity = predictions[:,1:] - predictions[:,:-1]\n",
    "speed = np.absolute(velocity)\n",
    "\n",
    "print(speed.shape)\n",
    "\n",
    "speed  = np.mean(speed, axis=-1)\n",
    "\n",
    "speed = np.reshape(speed, (map_d,map_d))\n",
    "\n",
    "print(speed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.imshow(speed, cmap='hot', interpolation='nearest')\n",
    "# plt.savefig(\"test.svg\", format=\"svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
